****************
WHAT I DID THIS SUMMER
- Made the code functional
The end-to-end code was incomplete at the beginning of the summer.  I put it together so that everything worked.

- wrote some filters to get the right results
After running 14 harvester strings through add_permutes and universal and specific filters and generating profiles for three language types (harvester sov, 
harvester vfinal, and miniaffixes), there were overgeneration and undergeneration errors in the profiles.  Modifying existing filters and creating new ones rectified this situation

- got regexy fv lists with backreferences
The feature/value combinations in the fv member of Filter instances initially had to be literal.  This was too constraining, though, when dealing with new iterator
features in choices files.  I made two changes to deal with these types of features.  The first was to allow for a regex-y like range feature in features where
[1-9] will match any digit 1 through 9.  The second change is the use of backreferences, which allows members of fv lists to talk to each other, in a manner of
speaking.  For example, in an fv list like ['and', 'verb-slot([1-9])_morph[1-9]_feat[1-9]_name:negation', r'verb-slot\1_order:after'], you have the group ([1-9])
in the first coordinand of the 'and' and then \1 backreference in the second coordinand ensures that the number in that place is the same that was in the group.
In this case it will ensure that it is negation that occurs after the verb.

- doco
All code that was used this summer is thoroughly documented now.

- speedups
In add_permutes, run_u_filters, and run_specific_filters, significant speed-ups were achieved by, among other things, not inserting every record that needed to be
inserted immediately, but instead waiting until there were 1000 or 10000 records to be added and inserting at that point.

- changed nf to be an affix
When moving on to the miniaffixes grammar there were undergeneration and overgeneration problems that could not be fixed by modifying filters.  Namely, stringmods
had a ChangeWord modification that changed tv to tv-nf and iv to iv-nf.  This was at odds with the miniaffixes grammar that treated -nf as a suffix, not as part
of a word.  Furthermore, miniaffixes is a grammar where the -nf suffix comes after the -neg suffix, but not vice-versa.  As such, a word like tv-neg-nf is allowable
but tv-nf-neg is not.  But since MatrixTDB considered tv-nf as an atomic word, it was not possible to produce tv-neg-nf.  nf is now an affix and the permutations
for 14 of the 18 harvester strings (all but neg2, neg4, ques2, and ques4) are entered into the database and run through all of the filters.
****************
Final status:

***State of the database***

MatrixTDB is still there, but I haven’t touched it since I created MatrixTDB2, which is what should be used going forward.  All the code I worked with is changed to work with MatrixTDB2 instead of MatrixTDB.

MatrixTDB2 has the following information in it:
The “easy” 14 harvester strings (the original 18 minus neg2, neg4, ques2, and ques4).
Those strings run through “everything” (add_permutes, run_specific_filters)
Three language types: original harvester grammar, harvester grammar changed to vfinal, miniaffixes

When generating profiles for those language types, the first two evaluated great.  Miniaffixes has an overgeneration problem of 52 strings.  This overgeneration must be related to the –nf as affix solution because it was good before I did that (see State of the Code)

***State of the code***
Immediate concerns I couldn’t resolve:
-	The overgeneration in miniaffixes.  After getting the –nf as an affix solution coded I’m not getting an overgeneration of 52 strings.
-	Speed of add_permutes.  I tried profiling, but didn’t find any obvious solutions.  I also forced Condor to give me a big machine, but it actually ran slower and stalled out with two seed strings remaining instead of just one.  I know David rebooted one of the big machines this morning and today add_permutes on the 14 harvester strings ran twice as fast, so I may kick this one off again tonight just for kicks.

Suggested fix for add_permutes speed:
-	My latest idea is that the problem isn’t time as it is space…when the code has to keep track of a list with 16 million elements it gets bogged down.  This could be fixed by turning uniq_permute into what python calls a generator.  A generator would work something like:

for perm in uniqPermGenerator(seedString):
	# run the perm through u filters and add those that pass all to a databse.

	The difference between this and what’s implemented is that the generator just generates the permutaitons as they’re needed, not all at one time.  Due to the multiple layers of permuting the words and the affixes and the affixes on the words, there should also be sub-generators built into the master generator.

***State of the repository***
Aside from the code, I’ve got the following folders and files added that have been my inputs and outputs
-	grammars/ - this directory has the four grammars I’ve been using: harv_orig, harv_vfinal, miniaffixes, and moreaffixes.  The harv_orig folder has the five source profiles I was using at the end.
-	dbroot/  - this contains the final three profiles I generated today: sov15 (harvester), vfinal15, miniaffixes15
-	addPerms.cmd – a template condor script for running add_permutes.  Described on the wiki
-	runSFltrs.cmd – a template condor script for running run_specific_filters.  Described on the wiki

********Everything below this point is in a better spot in the wiki************************
On these two pages:
http://wiki.delph-in.net/moin/MatrixTDBProcedures
http://wiki.delph-in.net/moin/MatrixTDBTables

There are two main things you might want to do with MatrixTDB: enter new harvester strings (and seed strings and permutations) or generate a profile for a language type.  These two main things are themselves made up of big steps which are explicated in detail below.  Here are how those two main things break down into
big steps.

****************
HOW TO ADD NEW HARVESTER STRINGS
- Create a source profile
- Import the source profile
- Add permutes
- Run specific filters
****************

****************
HOW TO GENERATE A PROFILE FOR A LANGUAGE TYPE
- Import the language type
- Generate a profile
****************

Here are how to perform those specific tasks, but first some notes on backing up and restoring the database.
****************
HOW TO BACK UP THE DATABASE
If you're not sure of the effect of what you are about to do, you may want to back up the database so that it can be restored if what you do doesn't go the way
you want.  To do this, run the following command:
	$ mysqldump -h capuchin.ling.washington.u -u username -p --result-file=resultfile dbname
	where the arguments are as folows:
		username - your username for the database
		resultfile - the name of the file you want to dump the backup to.
		dbname - the name of the database on capuchin to backup.  Currently MatrixTDB2 is the database we are using.

Note: This won't actually back up the data per se, but it will create a (very large) file full of SQL statements that can be used to restore the database to its
      state at the time of the dump.
****************

****************
HOW TO RESTORE A DUMP OF THE DATABASE
If you want to revert the database to a previous point, log in to the database and issue the following command:
	mysql> source filename
	where filename is the name of the file with the dump you want to revert to.
****************

****************
HOW TO ADD NEW HARVESTER STRINGS
- Create a source profile
- Import the source profile
- Add permutes of the harvester string in that source profile
- Run specific filters on those permutes
****************

****************
HOW TO GENERATE A PROFILE (the big task)
- Import the language type you want to generate the profile for
- Generate the profile (the small task)
- Evaluate the profile
****************

****************
HOW TO CREATE A SOURCE PROFILE
Source profiles (sometimes also called 'original source profiles') are what are used to bring the big, hairy mrs semantics into the database.  To create one:
- Create a flat file with one harvester string per line
- Start LKB and load the grammar you want to use to create the mrs semantics to import
- Start [incr_tsdb()] and process all the items in that file
****************

****************
HOW TO IMPORT A SOURCE PROFILE
Source profiles (sometimes also called 'original source profiles') are what are used to bring the big, hairy mrs semantics into the database.  When you have a 
[incr_tsdb()] profile that was created by processing a flat file of items you can use that to import a source profile into the database.  To do so:
- Create a file that has each harvester string in your profile on a line preceded by its mrs tag and a '@'  E.g., wo1@n1 iv
- Run the following command:
	$ python import_from_itsdb.py itsdbDir harvMrsFilename choicesFilename
	where the arguments are as follows:
	itsdbDir - the absolute directory of your [incr_tsdb()] directory.  Be sure to end it in a '/'
	harvMrsFilename - the name of the file you created above with mrs tags and harvester strings
	choicesFilename - the choices file of the grammar you used to create the profile
- The system will prompt for a username and password to the database
- The system may ask you if the tags you're adding really are new or if you want to replace the existing tags with the new semantics you are importing.  Answer
  appropriately.  If the system indicates you are changing some semantics, make sure that is what you want to do.
- The system will also ask you for a description of the source profile.  It can be up to 1000 characters.
- The system will import the profile and, if the choices file you used represents a language type not already in the database, will create a language type for
  that, too.  It will return the osp_id which you will need to add permutes and run specific filters and the language type, which you will need to add permutes
  run specific filters
****************

****************
HOW TO ADD PERMUTES
At this point you will have imported a profile with its harvester strings.  But a harvester string just yields to potentially millions of other possible strings
with the same semantics.  Specifically, each harvester string gives rise to seed strings which are then permuted and added to the database as string to be run
through specific filters.  (Earlier versions of MatrixTDB added all permutations which were run through universal filters and then specific filters, but more 
recently only those string/semantic tag combos that pass all universal filters are being added to the database.)  Seed strings are stored in a canonical form: words
in alphabetic order followed by prefixes in alphabetic order followed by suffixes in alphabetic order.  Permutations are then every possible permutation of the
words with every possible permutation of prefixes and suffixes on every word in every one of those permutations.  Seed strings are generated from harvester strings
by the stringmods in stringmod.py.  Here is how to generate all the permutations for an imported original source profile:
- Make sure stringmods is updated to meet your needs (optional)
- Create a condor file.  A template is in the repository named addPerms.cmd
	- change ospID to be the ID of the source profile you want to create permutes for
	- change username to be your username to the MatrixTDB database
	- change password to be your password to the MatrixTDB database
- Submit your command file to condor with the following line:
	$ condor_submit addPerms.cmd
- The process may take many hours depending on how many strings you have and how long they are.  Two ways to monitor the progress are to check the count of records
  in the result table or to check the .warning file from time to time.
****************

****************
HOW TO RUN SPECIFIC FILTERS
At this point you will have inserted every permutation that passes all universal filters for your source profile into the item_tsdb, parse, and result tables.  At
this point we need to record how the string/mrs combos fare when run through specific filters so that we can generate profiles for language types based on the
the results of those runs through filters.  Here's how:
- Make sure s_filters.py has the filters in it specific to the phenomena you are concerned with (optional)
- Create a condor file.  A template is in the repository named runSFltrs.cmd
	- change ospID to be the ID of the source profile you want to create permutes for
	- change username to be your username to the MatrixTDB database
	- change password to be your password to the MatrixTDB database
- Submit your command file to condor with the following line:
	$ condor_submit runSFltrs.cmd
- The process may take a few hours depending on how many permutations passed all universal filters in this OSP.  Two ways to monitor the progress are to check the
  count of records in the res_sfltr table or to check the .warning file from time to time.
****************

****************
HOW TO IMPORT A LANGUAGE TYPE
- Create a choices file from the customization system
- Run the following command:
	$ python sql_lg_type.py filename
	where filename is the full path and filename of the choices file you downloaded
- You will be prompted for the username and password to the database.
- If the language type already exists, it will return its ID.
- If the language type does not already exist, the system will ask if the language type is randomly created or purpose-built.  Answer r or p as appropriate.
  Unless you're just testing phenomena coverage of  MatrixTDB itself, yours is probably purpose-built.
- The system will prompt you for a short comment describing your language type.  You can enter the name of the language or the phenomena you're testing (e.g., 
  v-final), or whatever you feel is appropriate
- The system will then create a language type in the database and give you an lt_id (language type ID) that you can use to generate a [incr_tsdb()] profile for
  validation
****************

****************
HOW TO GENERATE A PROFILE (the small task)
- Make sure you have a language type ID that you want to generate the profile for.  You can get the ID by querying the lt table or by importing a language type 
  (see: HOW TO IMPORT A LANGUAGE TYPE)
- Run the following command:
	$ python generate_s_profile.py lt_id dbroot profilename
	where the three arguments are:
		- lt_id - the language type ID you are generating the profile for
		- dbroot - the absolute path where you want the profile created
		- profilename - the name of the profile to create
- You will be prompted for the username and password to the database.
- The system will generate two profiles in dbroot: [profilename] and [profilename]gold.  The gold version should be processed in [incr_tsdb()] and used as the gold
  standard
****************

****************
HOW TO EVALUATE A GENERATED PROFILE IN [INCR_TSDB()]
   * Start LKB and load the grammar that corresponds to the language type you generated the profile for
   * Start [incr_tsdb()] and set the dbroot to the directory where you generate the profile
   * Process all the items in the gold profile to create the gold standard
   * Set the gold profile to be the gold standard
   * Set readings and mrs to be the things compared in the intersection
   * Select the non-gold profile you are evaluating.
   * Run Compare->Detail.
   * If MatrixTDB output is good, the only differences you see should be that items with -1 readings in the gold should have 0 readings in MatrixTDB.
   * You can remove readings from the intersection and if MatrixTDB is right there will be no differences in Compare->Detail.
****************

****************
HOW TO WRITE A STRING MOD
In stringmod.py, the subclasses of the StringMod class define ways in which a harvester string can be modified to create a seed string.  All the stringmod 
instantiations that are applied to harvester strings are defined in the string_mods list near the end of the file.  To create a new stringmod, you need to
instantiate a new StringMod subclass and put it in that string_mods list.  Note that every possible combination of the string_mods list is run to create seeds from
harvester strings.  For example, there is a mod that adds the word p-nom and another that adds the word p-acc.  If these were the only two modifications, each
harvester string would result in four seed strings: one with p-nom, one with p-acc, one with both, and one with neither.)

To create a new instance of a StringMod subclass, you need to determine the subclass to instantiate as well as how to set the member values.  Here is an overview of
the subclasses:

StringModAddWord - adds a word to the string
StringModAddAff - adds an affix to the string (both as a prefix and in another string as an affix)
StringModDropWord - removes a word from a string if present
StringModChangeWord - changes a word in a string to another word.  Not currently used.

Once you've chosen the subclass to instantiate, call its constructor in the string_mods list, settings its arguments as appropriate:
mrs_id_list - a list of mrs_tags to which this stringmod applies.  Common groupings of these tags are defined in g.py.
word1 - for AddWord, AddAff, or DropWord, this is the word to add, the affix to add, or the word to drop.  For ChangeWord it is the word to change.
word2 - for ChangeWord only, this is the word to replace word1 with
****************

****************
HOW TO WRITE A FILTER
When you write a filter, first consider whether it is a universal filter or a specific filter.  A universal filter is something that can rule out a string
regardless of language type.  A specific filter is something that can rule out a string but is dependent on language type.  For example, ruling out any sentence
where the subject or object follows the verb in an sov language type.

To create a new filter, you need to create a new instance of a Filter subclass in either u_filters.filter_list or s_filters.filter_list.

To create a new instance of a Filter subclass, you need to determine the subclass to instantiate as well as how to set the member values.  Here is an overview of
the subclasses and how they interact with their re1 and re2 regular expression members.  These classes are defined in filters.py:
FalseFilter - always returns fail
MatchFilter - passes strings that contain re1, fails all others
NotMatchFilter - fails strings that contain re1, passes all others
IfFilter - fails strings that contain re1 but not re2, passes all others
IfNotFilter - fails strings that contain both re1 and re2, passes all others
OrFilter - passes strings that contain either re1, re2, or both, fails all others
other existing subclasses have been deprecated due to redundancies and should not be used, though more may be created later

After setting the Filter subclass and the regular expression members re1 and re2, if appropriate, set the other members of the Filter:

name - Just make sure it's unique among all filters

mrs_id_list - This is a list of the mrs tags that this filter should apply to.  For example, a filter that ensures that neg appears as a prefix or a suffix if 
			  inflectional negation is obligatroy would only apply to sentences with negative semantics.  You can either create your own list of strings that are
			  mrs tags or use one of the many lists created in g.py.
			  
comment - Describe what the Filter does

fv - Only relevant to specific filters.  The 'fv' stands for 'feature/value' and this is a formatted list of features and values that have to be set in a language
	 type for a filter to be relevant to that language type.  It has many aspects to it:
	 
	 length - The list can either be of length one or three or more.  If it is of length one it should be one feature/value pair.  If it is of length three or
			  more, its first element must be 'and' or 'or' and the later elements must be feature/value pairs or lists that must be formatted in the same ways
			  that a root fv list must be.
	  
	 feature/value pairs - With its combination of ands, ors, and feature/value pairs encapsulated in lists, the fv list corresponds to a set of feature/value pairs
						   that must be present for a filter to apply.  The feature value pairs are encoded as strings in the form 'feature:value'.  For example,
						   the fv list ['or', 'word-order:svo','word-order:sov','word-order:vso'] means that the filter applies if the word-order feature in the
						   language type is set to svo, sov, or vso.
	 
	 features with no value - Sometimes a feature/value pair appears with no value listed. In these cases, this is interpreted as meaning that that feature must
							  not be set to any value.  For example, the fv list ['or', 'q-inv:', 'q-inv-verb:main'] means that the filter applies if either
							  the q-inv-verb feature is set to main or the q-inv feature is absent in the choices file
	 
	 regex matches - With iterator features now in choices files, writing filters becomes more complicated because the developer cannot rely on a given feature
					 being named in a particular way.  For example, we never know what number the slot for negation will be placed in.  To address this, filters now
					 also apply if the feature/value pairs is a regular expression match with the choices file/language type.  For example the feature/value pair
					 verb-slot[1-9]_morph[1-9]_feat[1-9]_name:negation would apply if any feature whose name matches the range-filled regular expression before the
					 colon has a value of negation
	 
	 groups and backreferences - The regex matches go further.  Sometimes it's not enough just to match one feature to a regex but we want to be sure that there
								 are corresponding features that work together to ensure a certain phenomenon.  To this end the system also recognizes groups
								 and backreferences with a given filter.  Groups are indicated by sets of parentheses in regular expressions and backreferences
								 are escaped integers where the integer refers to the groups in the order they appear, beginning at 1.  For example, in the fv list
								 ['and', 'verb-slot([1-9])_morph[1-9]_feat[1-9]_name:negation', 'verb-slot\\1_order:before'], the backreference \\1 matches when the
								 number that actually appears there matches the number that actually appears after verb-slot in the prior feature.  This fv list
								 means that the filter applies when there is morphological negation that comes before the verb.
****************

****************
TABLES:

Tables used to group language types and filters.  Used by sql_lg_type, run_specific_filters, and generate_s_profile
-------------------
TABLE: feat_grp
DESCRIPTION: Puts combinations of features that are used in filters together.  Every feature/value combination found in filters and imported choices files (language
			 types) is entered as a singleton group (group with just one feature/value combo).  Every 'and' group of feature/value combos in filters is entered
			 as a group, i.e. with a common fg_grp_id.
COLUMNS: 
fg_id - primary key
fg_grp_id - the ID of the feature group
fg_feat - a feature
fg_value - the value of that feature
----------------
TABLE: filter
DESCRIPTION: Filter repository whose purpose is to match the names of the filters in s_filters.py to the IDs used as foreign keys in fltr_feat_grp.
COLUMNS:
filter_id - Primary key
filter_name - The name member of the filter in s_filters.py
filter_type - Either 'u' for universal or 's' for specific.  Now we're just using 's' since add_permutes only enters those items that pass all universal filters and
			  we're skipping run_u_filters.py which used this table for 'u' filters.
----------------
TABLE: fltr_feat_grp
DESCRIPTION: Intersection table that matches filters up to the feature groups to which they apply.  Used in combination with lt_feat_grp to get the filters that
			 apply to a language type
COLUMNS: 
ffg_id - Primary key
ffg_fltr_id - A foreign key to filter.filter_id
ffg_grp_id - A foreign key to feat_grp.fg_grp_id
----------------
TABLE: lt
DESCRIPTION: A repository of language types that have been imported, either by importing source profiles or separately.
COLUMNS:
lt_id - Primary key
lt_origin - Either 'r' or 'p'.  Indicates whether the language type was purpose-built or randomly-generated.
lt_comment - A user-enetered comment about what the language type is.
----------------
TABLE: lt_feat_grp
DESCRIPTION: Intersection table that matches language types up with the feature groups they have.  Used in combination with fltr_feat_grp to find filters that
			 are relevant to a language type.
COLUMNS:
lfg_id - Primary key
lfg_lt_id - Foreign key to lt.lt_id
lfg_grp_id - A foreign key to feat_grp.fg_grp_id
----------------
One final note on this set of tables: If you are debugging the system and trying to figure out why a given string was not grammatical, you will want to know what
specific filters it failed.  The way to do this is to run the query that generate_s_profile runs when determine which strings fail filters specific to the relevant
language type.  This query, in a slightly different from from that used by genereate_s_profile, is:
	SELECT filter_name FROM filter
		INNER JOIN res_sfltr ON filter_id = rsf_sfltr_id
        INNER JOIN fltr_feat_grp ON rsf_sfltr_id = ffg_fltr_id
		INNER JOIN lt_feat_grp ON ffg_grp_id = lfg_grp_id
	WHERE rsf_value = 0
		AND lfg_lt_id = [lt_id]
		AND rsf_res_id = [result_id]
where lt_id is the lt_id of the language type you are generating the profile for and result_id is the id of the result you are wondering about.  Be sure you find
the ID that matches item_tsdb.i_input (the string) and r.r_mrs (the semantic tag) you're wondering about.  This will get you a list of all specific filters that the
item failed.  If the item didn't fail any specific filters and still isn't appearing as a grammatical item in your profile it probably failed a universal filter (in which case you won't even have a result id for it.)

Tables used to store source profile information.  These tables are updated by import_from_itsdb.py and used by add_permutes.py
----------------
TABLE: harv_str
DESCRIPTION: Stores the harvester strings given when importing a source profile
COLUMNS:
hs_id - Primary key
hs_string - The harvester string
hs_mrs_tag - The mrs tag
hs_init_osp_id - The initial original source profile this string/tag combo was imported as a part of
hs_cur_osp_id - The most recent original source profile this string/tag combo was imported as a part of
----------------
TABLE: mrs
DESCRIPTION: Stores the mrs semantics for each of the mrs tags imported
COLUMNS:
mrs_id - Primary key
mrs_tag - Mrs semantic tag
mrs_value - The hairy mrs semantics produced in the source profile by [incr_tsdb()]
mrs_date - NULL
mrs_osp_id - The ID of the original source profile the mrs was imported from
mrs_current - 1 if this value of the tag is the current one
----------------
TABLE: orig_source_profile
DESCRIPTION: Stores import information about the source profiles
COLUMNS:
osp_orig_src_prof_id - Primary key
osp_developer_name - The developer who imported the source profile
osp_prod_date - The date the profile was imported
osp_orig_lt_id - The ID of the language type that produced the profile
osp_comment - A user-entered description of the source profile
----------------
TABLE: sp_item
DESCRIPTION: Mirrors the item file in the source profile with the addition of the spi_osp_id which is a foreign key to orig_source_profile.osp_orig_src_prof_id.
----------------
TABLE: sp_parse
DESCRIPTION: Mirrors the parse file in the source profile with the addition of the spp_osp_id which is a foreign key to orig_source_profile.osp_orig_src_prof_id.
----------------
TABLE: sp_result
DESCRIPTION: Mirrors the result file in the source profile with the addition of the spr_osp_id which is a foreign key to orig_source_profile.osp_orig_src_prof_id.
----------------

Tables updated by add_permutes.
----------------
TABLE: item_tsdb
DESCRIPTION: Mirrors the item file in a [incr_tsdb()] profile with the addition of i_osp_id which is a foreign key to orig_source_profile.osp_orig_src_prof_id.
----------------
TABLE: parse
DESCRIPTION: Mirrors the item file in a [incr_tsdb()] profile with the addition of p_osp_id which is a foreign key to orig_source_profile.osp_orig_src_prof_id.
----------------
TABLE: result
DESCRIPTION: Mirrors the result file in the source profile with the addition of the r_osp_id which is a foreign key to orig_source_profile.osp_orig_src_prof_id and 
			 r_esp_id which is NULL
----------------
TABLE: seed_str
DESCRIPTION: Stores the seed strings which are the harvester strings after having gone through stringmods stored in a canonical form: words, prefixes, suffixes,
			 with all three of those being in alphabetical order
COLUMNS:
seed_id - Primary key
seed_str_value - Seed string in canonical form
----------------
TABLE: str_lst
DESCRIPTION: Intersection table that maps seed strings to mrs tags
COLUMNS:
sl_id - Primary key
sl_mrs_tag - The semantic mrs tag this seed string belongs to.  Can serve as a foreign key to mrs.mrs_tag and result.r_mrs.
sl_seed_type - NULL
sl_seed_id - Foreign key to seed_str.seed_id
----------------

Table updated by run_specific_filters:
----------------
TABLE: res_sfltr
			 fails, not passes or does-not-applys.
DESCRIPTION: Intersection table that stores the 'result' of applying a specific filter to the 'result' in a [incr_tsdb()] profile.  Currently only used to store
COLUMNS:
rsf_id - Primary key.
rsf_res_id - Foreign key to result.r_result_id, parse.p_parse_id, and item_tsdb.i_id
rsf_sfltr_id - Foreign key to filter.filter_id
rsf_value - The 'result' of applying the filter to the 'result'.  0 for fail, 1 for pass, 2 for does-not-apply.
----------------

Table used by run_u_filters: (NB: not currently used)
----------------
TABLE: res_fltr
DESCRIPTION: Intersection table that stores the 'result' of applying a universal filter to the 'result' in a [incr_tsdb()] profile.  Was only being used to track
			 at most the first fail found for a given result.
COLUMNS:
rf_id - Primary key.
rf_res_id - Foreign key to result.r_result_id, parse.p_parse_id, and item_tsdb.i_id
rf_fltr_id - Foreign key to filter.filter_id
rf_value - The 'result' of applying the filter to the 'result'.  0 for fail, 1 for pass, 2 for does-not-apply.
----------------